{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMkgdV9WkXg8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, KBinsDiscretizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def preprocess_column(data, col_name, train=True, kbins=None, scaler=None):\n",
        "    \"\"\"Decides the best preprocessing method for a column and applies it.\"\"\"\n",
        "\n",
        "    # If number of unique values is above a threshold (let's say 20), consider bucketing\n",
        "    if len(data[col_name].unique()) > 20:\n",
        "        if train:\n",
        "            kbins = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
        "            data[col_name] = kbins.fit_transform(data[[col_name]])\n",
        "        else:\n",
        "            data[col_name] = kbins.transform(data[[col_name]])\n",
        "    # Else if variance is high, apply StandardScaler if mean is near 0, else MinMaxScaler\n",
        "    elif data[col_name].mean() < 5 and data[col_name].var() > 1:\n",
        "        if train:\n",
        "            scaler = StandardScaler()\n",
        "            data[col_name] = scaler.fit_transform(data[[col_name]])\n",
        "        else:\n",
        "            data[col_name] = scaler.transform(data[[col_name]])\n",
        "    elif data[col_name].var() > 1:\n",
        "        if train:\n",
        "            scaler = MinMaxScaler()\n",
        "            data[col_name] = scaler.fit_transform(data[[col_name]])\n",
        "        else:\n",
        "            data[col_name] = scaler.transform(data[[col_name]])\n",
        "    return data, kbins, scaler\n",
        "\n",
        "def preprocess_data(X_train, X_val, X_test):\n",
        "    \"\"\"Main preprocessing function.\"\"\"\n",
        "\n",
        "    kbins_dict = {}\n",
        "    scaler_dict = {}\n",
        "\n",
        "    # Preprocess each column\n",
        "    for col in X_train.columns:\n",
        "        X_train, kbins, scaler = preprocess_column(X_train, col, train=True)\n",
        "        kbins_dict[col] = kbins\n",
        "        scaler_dict[col] = scaler\n",
        "\n",
        "        X_val, _, _ = preprocess_column(X_val, col, train=False, kbins=kbins_dict[col], scaler=scaler_dict[col])\n",
        "        X_test, _, _ = preprocess_column(X_test, col, train=False, kbins=kbins_dict[col], scaler=scaler_dict[col])\n",
        "\n",
        "    return X_train, X_val, X_test\n",
        "\n",
        "# Placeholder dataframes (to avoid errors in the code block)\n",
        "X_train = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'b': [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4]})\n",
        "X_val = pd.DataFrame({'a': [1, 2, 3, 4, 5], 'b': [-2, -1, 0, 1, 2]})\n",
        "X_test = pd.DataFrame({'a': [6, 7, 8, 9, 10], 'b': [3, 4, 5, 6, 7]})\n",
        "\n",
        "# Preprocess placeholder data\n",
        "X_train, X_val, X_test = preprocess_data(X_train, X_val, X_test)\n",
        "X_train, X_val, X_test\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n"
      ],
      "metadata": {
        "id": "naDmguEInxA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder for best models\n",
        "best_models = {}\n",
        "\n",
        "# Logistic Regression\n",
        "param_grid_lr = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "clf_lr = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid_lr, cv=3)\n",
        "clf_lr.fit(X_train, y_train)\n",
        "best_models['lr'] = clf_lr.best_estimator_\n",
        "\n",
        "# Random Forest\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "clf_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=3)\n",
        "clf_rf.fit(X_train, y_train)\n",
        "best_models['rf'] = clf_rf.best_estimator_\n",
        "\n",
        "# Gradient Boosting (using XGBoost)\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'learning_rate': [0.01, 0.1, 0.5],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "clf_xgb = GridSearchCV(XGBClassifier(), param_grid_xgb, cv=3)\n",
        "clf_xgb.fit(X_train, y_train)\n",
        "best_models['xgb'] = clf_xgb.best_estimator_\n",
        "\n",
        "# SVM\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "clf_svm = GridSearchCV(SVC(probability=True), param_grid_svm, cv=3)\n",
        "clf_svm.fit(X_train, y_train)\n",
        "best_models['svm'] = clf_svm.best_estimator_\n",
        "\n",
        "# Neural Network\n",
        "param_grid_mlp = {\n",
        "    'hidden_layer_sizes': [(50,), (100,)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam']\n",
        "}\n",
        "clf_mlp = GridSearchCV(MLPClassifier(max_iter=1000), param_grid_mlp, cv=3)\n",
        "clf_mlp.fit(X_train, y_train)\n",
        "best_models['mlp'] = clf_mlp.best_estimator_\n",
        "\n",
        "# CatBoost (using randomized search because of the large hyperparameter space)\n",
        "param_dist_cat = {\n",
        "    'iterations': [50, 100, 150],\n",
        "    'depth': [3, 5, 7, 10],\n",
        "    'learning_rate': [0.01, 0.1, 0.5],\n",
        "    'loss_function': ['Logloss']\n",
        "}\n",
        "clf_cat = RandomizedSearchCV(CatBoostClassifier(verbose=0), param_dist_cat, cv=3, n_iter=10)\n",
        "clf_cat.fit(X_train, y_train)\n",
        "best_models['cat'] = clf_cat.best_estimator_\n"
      ],
      "metadata": {
        "id": "KjAI_s3kks4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voting_clf = VotingClassifier(estimators=[('lr', best_models['lr']),\n",
        "                                          ('rf', best_models['rf']),\n",
        "                                          ('xgb', best_models['xgb']),\n",
        "                                          ('svm', best_models['svm']),\n",
        "                                          ('mlp', best_models['mlp']),\n",
        "                                          ('cat', best_models['cat'])],\n",
        "                              voting='soft')\n",
        "voting_clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "V2c7F5K6nrzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X, y):\n",
        "    y_pred = model.predict(X)\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    precision = precision_score(y, y_pred)\n",
        "    recall = recall_score(y, y_pred)\n",
        "    f1 = f1_score(y, y_pred)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluating on train, val, and test datasets\n",
        "train_metrics = evaluate_model(voting_clf, X_train, y_train)\n",
        "val_metrics = evaluate_model(voting_clf, X_val, y_val)\n",
        "test_metrics = evaluate_model(voting_clf, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "Jbzgq8AOn7f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calibrated_clf = CalibratedClassifierCV(voting_clf, method='sigmoid', cv='prefit')\n",
        "calibrated_clf.fit(X_val, y_val)\n",
        "\n",
        "# Evaluate the calibrated classifier on the test set\n",
        "test_metrics_calibrated = evaluate_model(calibrated_clf, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "ZiWl4M1Kn9tW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}