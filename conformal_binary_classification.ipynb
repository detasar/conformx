{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyswarm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJcOZXOGWIiX",
        "outputId": "0462ad8e-3d9a-4245-c343-f8cdd5df041f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyswarm\n",
            "  Downloading pyswarm-0.6.tar.gz (4.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyswarm) (1.23.5)\n",
            "Building wheels for collected packages: pyswarm\n",
            "  Building wheel for pyswarm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyswarm: filename=pyswarm-0.6-py3-none-any.whl size=4464 sha256=3de26deb1cd529dab4c2845b48e92bd775e770f547d8e802718f5e8609be0326\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/40/62fa158f497f942277cbab8199b05cb61c571ab324e67ad0d6\n",
            "Successfully built pyswarm\n",
            "Installing collected packages: pyswarm\n",
            "Successfully installed pyswarm-0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.utils import resample\n",
        "\n",
        "import scipy.stats as stats"
      ],
      "metadata": {
        "id": "ClXZm3DqWH78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Ayarları yapılandıralım\n",
        "plt.style.use('seaborn-darkgrid')\n",
        "sns.set_palette('colorblind')\n",
        "\n",
        "\n",
        "# Veri kümesini oluşturalım\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10,\n",
        "                           random_state=42, flip_y=0.03, class_sep=0.7)\n",
        "\n",
        "# Eğitim, kalibrasyon ve test kümelerini ayarlayalım\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
        "x_calibration, x_test, y_calibration, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FV3MyroV8ax",
        "outputId": "51210237-f876-4781-d032-d864fd72680b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-e5b634838318>:17: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-darkgrid')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "yrDPpYcSWUGF",
        "outputId": "ffeac84f-a36d-48e9-90d5-9a6e503d797e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVK2F2GNHf0x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import concurrent.futures\n",
        "from sklearn.metrics import (\n",
        "    log_loss,\n",
        "    hinge_loss,\n",
        "    accuracy_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        ")\n",
        "from pyswarm import pso\n",
        "from typing import List, Tuple, Callable, Dict, Optional, Union, Any\n",
        "\n",
        "class ConformalPredictionOptimizer:\n",
        "    def __init__(self, model: Callable):\n",
        "        \"\"\"\n",
        "        Initializes the optimizer object with a machine learning model.\n",
        "\n",
        "        :param model: A trained binary classification model with a predict_proba method.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "\n",
        "    @staticmethod\n",
        "    def squared_hinge_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Computes the squared hinge loss.\n",
        "\n",
        "        :param y_true: Array of true binary labels.\n",
        "        :param y_pred: Array of predicted probabilities.\n",
        "        :return: The mean squared hinge loss.\n",
        "        \"\"\"\n",
        "        loss = np.maximum(0, 1 - y_true * y_pred)\n",
        "        return np.mean(loss ** 2)\n",
        "\n",
        "    def objective_function(self, weights: np.ndarray, loss_function: Callable, x_calib: np.ndarray, y_calib: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Computes the objective function value for given weights and a loss function.\n",
        "\n",
        "        :param weights: Array of weights to apply to the model's predictions.\n",
        "        :param loss_function: A callable loss function to evaluate the objective value.\n",
        "        :param x_calib: Calibration feature data.\n",
        "        :param y_calib: Calibration target data.\n",
        "        :return: The total loss computed with the given loss function.\n",
        "        \"\"\"\n",
        "        self.validate_inputs(loss_function, x_calib, y_calib)\n",
        "\n",
        "        if len(weights) != 2:\n",
        "            raise ValueError(\"Weights array must be of length 2.\")\n",
        "\n",
        "        weighted_proba = weights[0] * self.model.predict_proba(x_calib)[:, 1] + weights[1]\n",
        "        decision_function_output = 2 * weighted_proba - 1  # Adjusted predictions\n",
        "        return np.mean(loss_function(y_calib, decision_function_output))\n",
        "\n",
        "    def validate_inputs(self, loss_function: Callable, x_calib: np.ndarray, y_calib: np.ndarray) -> None:\n",
        "        if not callable(loss_function):\n",
        "            raise ValueError(\"loss_function must be a callable function.\")\n",
        "\n",
        "        if not isinstance(x_calib, np.ndarray) or not isinstance(y_calib, np.ndarray):\n",
        "            raise ValueError(\"x_calib and y_calib must be numpy arrays.\")\n",
        "\n",
        "        if len(x_calib) != len(y_calib):\n",
        "            raise ValueError(f\"x_calib and y_calib must be of the same length. Got lengths {len(x_calib)} and {len(y_calib)}.\")\n",
        "\n",
        "    def optimize(self, x_train: np.ndarray, y_train: np.ndarray, x_calib: np.ndarray, y_calib: np.ndarray, x_test: np.ndarray, lower_bound: Tuple[float, float] = (0, 0), upper_bound: Tuple[float, float] = (1, 1)) -> Tuple[List[Tuple[float, float]], str]:\n",
        "        \"\"\"\n",
        "        Optimizes the conformal prediction model and returns the best intervals.\n",
        "\n",
        "        :param x_train: Training feature data.\n",
        "        :param y_train: Training target data.\n",
        "        :param x_calib: Calibration feature data.\n",
        "        :param y_calib: Calibration target data.\n",
        "        :param x_test: Test feature data.\n",
        "        :param lower_bound: Lower bounds for weights.\n",
        "        :param upper_bound: Upper bounds for weights.\n",
        "        :return: The optimized intervals and the corresponding best loss function name.\n",
        "        \"\"\"\n",
        "        self.model.fit(x_train, y_train)\n",
        "        best_score = float('inf')\n",
        "        best_intervals = None\n",
        "        best_loss = None\n",
        "\n",
        "        loss_functions = {\n",
        "            'log_loss': log_loss,\n",
        "            'hinge_loss': hinge_loss,\n",
        "            'squared_hinge_loss': self.squared_hinge_loss\n",
        "        }\n",
        "\n",
        "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "            futures = [\n",
        "                executor.submit(pso, self.objective_function, lower_bound, upper_bound, args=(loss_function, x_calib, y_calib))\n",
        "                for _, loss_function in loss_functions.items()\n",
        "            ]\n",
        "\n",
        "            for future, loss_name in zip(concurrent.futures.as_completed(futures), loss_functions.keys()):\n",
        "                try:\n",
        "                    weights, score = future.result()\n",
        "                except Exception as e:\n",
        "                    raise RuntimeError(f\"Optimization failed with loss function {loss_name}. Reason: {str(e)}\")\n",
        "\n",
        "                test_proba = weights[0] * self.model.predict_proba(x_test)[:, 1] + weights[1]\n",
        "                calib_proba = weights[0] * self.model.predict_proba(x_calib)[:, 1] + weights[1]\n",
        "\n",
        "                calib_scores = [loss_functions[loss_name]([y], [2 * proba - 1]) for y, proba in zip(y_calib, calib_proba)]\n",
        "                intervals = [\n",
        "                    (np.mean(calib_scores <= score), np.mean(calib_scores < score) + (1 / len(calib_scores)))\n",
        "                    for score in [loss_functions[loss_name]([y], [2 * proba - 1]) for proba in test_proba]\n",
        "                ]\n",
        "\n",
        "                if score < best_score:\n",
        "                    best_score = score\n",
        "                    best_intervals = intervals\n",
        "                    best_loss = loss_name\n",
        "\n",
        "        return best_intervals, best_loss\n",
        "\n",
        "\n",
        "def evaluate_intervals(intervals: List[Tuple[float, float]], y_test: np.ndarray, model_predictions: np.ndarray) -> Tuple[Dict[str, Union[float, np.ndarray]], Optional[Dict[str, Union[float, np.ndarray]]]]:\n",
        "    \"\"\"\n",
        "    Evaluates the model performance based on the intervals and returns the performance metrics.\n",
        "\n",
        "    :param intervals: The predicted intervals for the test data.\n",
        "    :param y_test: The true labels for the test data.\n",
        "    :param model_predictions: The model's predictions on the test data.\n",
        "    :return: Performance metrics for reliable and non-reliable predictions.\n",
        "    \"\"\"\n",
        "    reliable_indices = [i for i, interval in enumerate(intervals) if interval[0] <= 0.5]\n",
        "    non_reliable_indices = [i for i, interval in enumerate(intervals) if interval[0] > 0.5]\n",
        "\n",
        "    reliable_y_test = np.take(y_test, reliable_indices)\n",
        "    non_reliable_y_test = np.take(y_test, non_reliable_indices)\n",
        "\n",
        "    reliable_predictions = np.take(model_predictions, reliable_indices)\n",
        "    non_reliable_predictions = np.take(model_predictions, non_reliable_indices)\n",
        "\n",
        "    def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Union[Dict[str, Union[float, np.ndarray]], str]:\n",
        "        if len(y_true) < 2 or len(np.unique(y_true)) < 2:\n",
        "            return \"Cannot calculate metrics with less than two classes\"\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy_score(y_true, y_pred),\n",
        "            'recall': recall_score(y_true, y_pred),\n",
        "            'precision': precision_score(y_true, y_pred),\n",
        "            'f1': f1_score(y_true, y_pred),\n",
        "            'roc_auc': roc_auc_score(y_true, y_pred),\n",
        "            'conf_matrix': confusion_matrix(y_true, y_pred),\n",
        "        }\n",
        "\n",
        "    reliable_metrics = calculate_metrics(reliable_y_test, reliable_predictions)\n",
        "    non_reliable_metrics = calculate_metrics(non_reliable_y_test, non_reliable_predictions) if non_reliable_y_test.size else \"No non-reliable predictions to evaluate.\"\n",
        "\n",
        "    return reliable_metrics, non_reliable_metrics\n",
        "\n",
        "\n",
        "# Usage:\n",
        "# Make sure model, x_train, y_train, x_calibration, y_calibration, x_test, and y_test are defined.\n",
        "optimizer = ConformalPredictionOptimizer(model)\n",
        "best_intervals, best_loss = optimizer.optimize(x_train, y_train, x_calibration, y_calibration, x_test, lower_bound=(0, 0), upper_bound=(1, 1))\n",
        "\n",
        "# Evaluate intervals\n",
        "reliable_metrics, non_reliable_metrics = evaluate_intervals(best_intervals, y_test, model.predict(x_test))\n",
        "\n",
        "# Printing the Results:\n",
        "print(f\"Best Loss Function: {best_loss}\")\n",
        "print(\"Performance Metrics for Reliable Predictions:\")\n",
        "print(reliable_metrics)\n",
        "print(\"\\nPerformance Metrics for Non-Reliable Predictions:\")\n",
        "print(non_reliable_metrics)\n"
      ]
    }
  ]
}